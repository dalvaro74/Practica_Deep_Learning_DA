{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"mixed_regresion.ipynb","provenance":[{"file_id":"1iu7gZyGdnPPHy6rwo2QcibegVzg2LqGF","timestamp":1580562843553}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"D-pdMy4xE146","colab_type":"text"},"source":["# Resumen\n","En este notebook vamos a partir del dataset (airbnb-images-clean.csv)\n","\n","Llevaremos a cabo la misma limpieza y seleccion de caracteristicas que hicimos en el caso de la practica del modulo de Machine Learning y que ya hemos llevado a cabo en el notebook nn_regresion_01.\n","\n","Asi mismo usaremos ese mismo dataset para obtener un array con las imagenes como en el caso del notebook cnn_regresion.\n","\n","En este caso vamos a usar el codigo del fichero models.py extraido del ejemplo indicado por el profesor para la realizacion de la practica.\n","\n","https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n","\n","Dicho código ha sido modificado para adaptarse a nuestras necesidades.\n","\n","La red cnn se alimentara con las imagenes obtenidas y la red nn con los datos(categoricos y numericos).\n","A su vez la salida de cada una de estas redes (las cuales obviamente no tendran la parte de regresion) alimentara a una red Neuronal tradicional (Fully Conected) que se encargara de llevar a cabo la parte de la predicción.\n","\n","Como en el caso de cnn_regresion, para la generacion del array de imagenes es necesario un entorno de Colab de tipo \"High Ram Runtime\" el cual como ya indicamos anteriormente no siempre esta disponible. Esto ha provocado que no hayamos podido hacer todas las pruebas que nos hubiera gustado tuneando nuestro modelo mixto."]},{"cell_type":"markdown","metadata":{"id":"ntAkUgE-HUHv","colab_type":"text"},"source":["# Montar directorio Drive en colab\n","En primer lugar vamos a montar el directorio de Drive para poder utilizar los ficheros de carga y para guardar de manera permanente otros ficheros de interes"]},{"cell_type":"code","metadata":{"id":"0sUD1K2pkoHE","colab_type":"code","outputId":"7fb9ab17-784b-46ab-a943-cb4997f76df4","executionInfo":{"status":"ok","timestamp":1581065832189,"user_tz":-60,"elapsed":31267,"user":{"displayName":"DANIEL ALVARO PARICIO","photoUrl":"","userId":"03090964372750420749"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GLVoDM6axpHI","colab_type":"code","colab":{}},"source":["# Para forzar que nos pasen a un entorno de tipo \"High Ram Runtime\"\n","#a = []\n","#while(1):\n","    #a.append('1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tty3CcYKIKZ0","colab_type":"code","colab":{}},"source":["from os.path import join\n","ROOT = \"/content/gdrive\"\n","PROJ = \"My Drive/Public/Practica_Deep_Learning_DA\" \n","PROJECT_PATH = join(ROOT, PROJ)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WjSj4ulcDY5","colab_type":"code","outputId":"daa868f3-85a8-46b9-d99f-95fe6fb4ba7f","executionInfo":{"status":"ok","timestamp":1581065834026,"user_tz":-60,"elapsed":33019,"user":{"displayName":"DANIEL ALVARO PARICIO","photoUrl":"","userId":"03090964372750420749"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["!ls \"/content/gdrive/My Drive/Public/Practica_Deep_Learning_DA\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["00-Explicacion_Practica.ipynb  descarga_imagenes.ipyn  nn_regresion_01.ipynb\n","cnn_regresion.ipyn\t       images\t\t       nn_regresion_02.ipynb\n","data\t\t\t       mixed_regresion.ipynb   util\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j7eCt-0NE14-","colab_type":"text"},"source":["# Librerías y funciones\n","\n","Lo primero es cargar las librerías y funciones necesarias."]},{"cell_type":"code","metadata":{"id":"9-NYFLmHE15A","colab_type":"code","outputId":"49471f45-cef8-4be5-e3d3-cac7d1a83bc7","executionInfo":{"status":"ok","timestamp":1581065836314,"user_tz":-60,"elapsed":35289,"user":{"displayName":"DANIEL ALVARO PARICIO","photoUrl":"","userId":"03090964372750420749"}},"colab":{"base_uri":"https://localhost:8080/","height":81}},"source":["import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","%matplotlib inline\n","\n","cm = plt.cm.RdBu\n","cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","#Para incluir en Colab las funciones del package utilidades creado por mi\n","from importlib.machinery import SourceFileLoader\n","import sys\n","\n","missing_values_table = SourceFileLoader('missing_values_table', join(PROJECT_PATH, 'util/utilidades.py')).load_module()\n","change_cat_to_other = SourceFileLoader('change_cat_to_other', join(PROJECT_PATH, 'util/utilidades.py')).load_module()\n","create_nn = SourceFileLoader('create_nn', join(PROJECT_PATH, 'util/models.py')).load_module()\n","create_cnn = SourceFileLoader('create_cnn', join(PROJECT_PATH, 'util/models.py')).load_module()\n","\n","\n","sys.path.append('/content/gdrive/My\\ Drive/Public/Practica_Deep_Learning_DA/util')\n","from missing_values_table import missing_values_table\n","from change_cat_to_other import change_cat_to_other\n","from create_nn import create_nn\n","from create_cnn import create_cnn\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"rRGUu7Qm3Utj","colab_type":"text"},"source":["# Analisis exploratorio"]},{"cell_type":"markdown","metadata":{"id":"aJ6P6EfIE15I","colab_type":"text"},"source":["## Datos de entrada\n","\n","Cargamos los datos del fichero de airbnb reducido.\n","Este fichero contiene 14870 observaciones y 89 variables"]},{"cell_type":"code","metadata":{"id":"jCh_i2tCE15I","colab_type":"code","colab":{}},"source":["full_airbnb_images = pd.read_csv(join(PROJECT_PATH,'data/airbnb-images-clean.csv'),sep=';', decimal='.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mj4FKY-uE15O","colab_type":"text"},"source":["## Outliers\n","Antes de nada, vamos a hacer una pequeña limpieza de los outliers de precio,"]},{"cell_type":"code","metadata":{"id":"VYhsc0uGE15P","colab_type":"code","colab":{}},"source":["full_airbnb_images = full_airbnb_images[(full_airbnb_images['Price']>10) & (full_airbnb_images['Price']<200)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZVCorIR9YhH","colab_type":"text"},"source":["## Limpieza general"]},{"cell_type":"markdown","metadata":{"id":"kIvMpLz3E15X","colab_type":"text"},"source":["Antes de llevar a cabo el split entre Trainintg y Test eliminamos las columnas que tenemos claro que no van a ser utiles para nuestro objetivo:\n","1. **Las que contienen URLs**: Listing Url: drop_url  \n","2. **Los Ids y lo relativo al Scrape realizado**: drop_id_scrape\n","3. **Nombres y comentarios**:drop_comments\n","4. **Direcciones**: A la vista de la informacion que contienen las variables de direccion podemos dropear varias de ellas por diversos motivos (sin que tengamos que dividir previamente en Train Test). Demasiado genericas: City, State, Market, Smart Location, Country Code, Country, Jurisdiction Names. Demasiado concretas: Street, Latitude, Longitude y Geolocation. Demasiado ruido o demasiados registros nulos: Neighbourhood, Host Location, Host Neighbourhood. Por ultimo, Zipcode es una variable que para representar la direccion no me parece la mas adecuada debido a que aunque es un numero, deberia ser tratado como una variable categorica. Ademas contiene bastante ruido, una cantidad no despreciable de nulos y tambien es demasiado concreta (506 valores unicos)     Por tanto y para la evaluacion del modelo deberemos barajar cual de las dos opciones que quedan es la mejor para representar la \"zona\" en la que se encuentra el piso ( Neighbourhood Cleansed o Neighbourhood Group Cleansed, las cuales obviamente van a estar fuertemente correladas), pero esto debera hacerse una vez separado el dataset, para que los datos de Test no influyan en la decision. (En cualquier caso sera necesario hacer un trabajo de limpieza y categorizacion con la variable elegida):drop_address\n","5. **Informascion relativa al Hospedador**: drop_host\n","6. **Nuevo campo incluido en la descarga de imagenes**: image_path"]},{"cell_type":"code","metadata":{"id":"qS30ABR6E15Z","colab_type":"code","colab":{}},"source":["drop_url = np.array(['Listing Url', 'Thumbnail Url', 'Medium Url', 'Picture Url', 'XL Picture Url', 'Host URL',\n","                     'Host Thumbnail Url','Host Picture Url'])\n","full_airbnb_images.drop(drop_url, axis=1, inplace=True)\n","\n","drop_id_scrape = np.array(['ID', 'Scrape ID', 'Last Scraped', 'Host ID', 'Calendar last Scraped'])\n","full_airbnb_images.drop(drop_id_scrape, axis=1, inplace=True)\n","\n","\n","drop_comments = np.array(['Name', 'Summary', 'Space', 'Description', 'Neighborhood Overview', 'Notes',\n","                     'Transit','Access', 'Interaction', 'House Rules', 'Host Name', 'Experiences Offered',\n","                         'Host About', 'Amenities', 'Features'])\n","\n","full_airbnb_images.drop(drop_comments, axis=1, inplace=True)\n","\n","drop_address = np.array(['Host Location', 'Host Neighbourhood', 'Neighbourhood', 'Street', 'Zipcode', \n","    'City', 'State', 'Market', 'Smart Location','Country Code', 'Country', 'Latitude', \n","                         'Longitude', 'Jurisdiction Names', 'Geolocation'])\n","\n","full_airbnb_images.drop(drop_address, axis=1, inplace=True)\n","\n","drop_host = np.array(['Host Since', 'Host Response Time', 'Host Response Rate', 'Host Acceptance Rate', \n","    'Host Listings Count', 'Host Total Listings Count','Host Verifications', 'Calculated host listings count'])\n","\n","full_airbnb_images.drop(drop_host, axis=1, inplace=True)\n","\n","#Por ultimo borramos la nueva columna incluida en el proceso de descarga de imagenes image_path\n","full_airbnb_images.drop(['image_path'], axis=1, inplace=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Pzs4WhNE15b","colab_type":"text"},"source":["Por ultimo eliminamos varios campos sueltos por los siguientes motivos:\n","* **Square Feet**: Contiene 96% observaciones null\n","* **Weekly Price**: Contiene 76% observaciones null\n","* **Monthly Price**: Contiene 76% obsevaciones null\n","* **Has Availability**: Contiene 99% observaciones null\n","* **First Review**: No creemos que aporte informacion util para el calculo del precio\n","* **Last Review**: No creemos que aporte informacion util para el calculo del precio\n","* **Calendar Updated**: No creemos que aporte informacion util para el calculo del precio\n","* **License**: Contiene 98% observaciones null\n","* **Bed Type**: Casi el 99% de las camas son del mismo tipo (Real Bed)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"3JjXpStPE15c","colab_type":"code","colab":{}},"source":["drop_varios = np.array(['Square Feet', 'Weekly Price', 'Monthly Price', 'Has Availability', 'First Review', 'Last Review',\n","                     'Calendar Updated','License', 'Bed Type'])\n","\n","full_airbnb_images.drop(drop_varios, axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrTw1jdRCrHW","colab_type":"code","outputId":"10be1129-822f-4857-a94c-81f48419303a","executionInfo":{"status":"ok","timestamp":1581065837947,"user_tz":-60,"elapsed":36751,"user":{"displayName":"DANIEL ALVARO PARICIO","photoUrl":"","userId":"03090964372750420749"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["full_airbnb_images.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11476, 29)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"_XzcdmOEE15n","colab_type":"text"},"source":["**Tras la limpieza inicial nos hemos quedado con 29 caracteristicas (y el target).**"]},{"cell_type":"markdown","metadata":{"id":"pIVWdnNaBJEB","colab_type":"text"},"source":["## Generacion del array de Imagenes\n","En lugar de volver a recorrer todas las imagenes como en el caso de cnn_regresion vamos a usar el fichero que creamos, en ese mismo notebook, con los datos del array de imagenes"]},{"cell_type":"code","metadata":{"id":"RbEhBBz7BsYq","colab_type":"code","colab":{}},"source":["from numpy import load\n","dict_data = load(join(PROJECT_PATH,'data/data.npz'))\n","# extract the first array\n","data = dict_data['arr_0']\n","\n","# Normalizamos los pixeles de las imagenes al rango [0,1]\n","data = data/255.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w0LotzQFErf8","colab_type":"code","colab":{}},"source":["data.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGuunPJaE15o","colab_type":"text"},"source":["## Separación Train Test\n","\n","Llevaremos a cabo la separacion conjunta del dataset y del array de imagenes para que coincidan en el mismo orden"]},{"cell_type":"code","metadata":{"id":"aszrwJxcE15p","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","(train, test, trainImagesX, testImagesX) = train_test_split(full_airbnb_images, data, test_size=0.25, random_state=55)\n","print(f'Dimensiones del dataset de training: {trainAttrX.shape}')\n","print(f'Dimensiones del dataset de test: {trainAttrX.shape}')\n","# Guardamos\n","train.to_csv('/content/gdrive/My Drive/Public/Practica_Deep_Learning_DA/data/train_clean.csv', sep=';', decimal='.', index=False)\n","test.to_csv('/content/gdrive/My Drive/Public/Practica_Deep_Learning_DA/data/test_clean.csv', sep=';', decimal='.', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"52wlKPUHE15r","colab_type":"text"},"source":["**Tengamos en cuenta que a partir de ahora todo el analisis exploratorio y selección de caracteristicas se hara sobre el dataset de entrenamiento.**\n","**Posteriormente todas transformaciones llevadas a cabo en dicho dataset se deberan implementar en el de Test.**"]},{"cell_type":"markdown","metadata":{"id":"BX2caflM3_kI","colab_type":"text"},"source":["## Limpieza y preprocesado de variables categoticas"]},{"cell_type":"markdown","metadata":{"id":"GvPW4It7E15s","colab_type":"text"},"source":["Empecemos tratando las variables de vecindario (Neighbourhood Cleansed y Neighbourhood Group Cleansed)"]},{"cell_type":"code","metadata":{"id":"Ai4_sc9SE15s","colab_type":"code","colab":{}},"source":["print(len(train['Neighbourhood Cleansed'].unique()))\n","print(train['Neighbourhood Cleansed'].isna().sum())\n","with pd.option_context(\"display.max_rows\", 1000):\n","    print(train['Neighbourhood Cleansed'].value_counts())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VptN_M2QE15u","colab_type":"code","colab":{}},"source":["print(len(train['Neighbourhood Group Cleansed'].unique()))\n","print(train['Neighbourhood Group Cleansed'].isna().sum())\n","with pd.option_context(\"display.max_rows\", 1000):\n","    print(train['Neighbourhood Group Cleansed'].value_counts())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJsl9aJfE15x","colab_type":"text"},"source":["A la vista de los datos anteriores tenemos la siguiente información:\n","- La variable \"Neighbourhood Cleansed\" contiene 390 categorias y ningun valor null.\n","- La variable \"Neighbourhood Group Cleansed contiene 46 categorias y 526 celdas null.En lugar de imputar los nulls a la categoria \"Other\" lo rellenamos con el valor de la columna \"Neighbourhood Cleansed\""]},{"cell_type":"code","metadata":{"id":"35DgxshYE15y","colab_type":"code","colab":{}},"source":["train['Neighbourhood Group Cleansed'].fillna(train['Neighbourhood Cleansed'], inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pafYMV8NxwtP","colab_type":"text"},"source":["Es cierto que ahora hemos pasado de 46 categorias a 230, pero creemos que esta distribucion es mas realista que la de incluir tantos pisos en la categoria de Other.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"okVCS3fVE152","colab_type":"text"},"source":["A continuación trataremos las otras tres variables categoricas que nos quedan (Property Type, Room Type y Cancellation Policy)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"uGiOj18CE15_","colab_type":"code","colab":{}},"source":["print(train['Property Type'].value_counts())\n","print(train['Room Type'].value_counts())\n","print(train['Cancellation Policy'].value_counts())\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jfayd6aUE16B","colab_type":"text"},"source":["Vamos a dejar las categorias de Property Type en \"Apartment\", \"House\", \"Condominium\", \"Bed & Breakfast\", \"Loft\", \"Dorm\", \"Guesthouse\", \"Chalet\", \"Townhouse\", \"Hostel\" y \"Villa\" que representan mas del 95% del total y todas las demas las incluiremos en la categoría \"Other\".\n","Para ello usaremos la funcion change_cat_to_other del pakcage utilidades"]},{"cell_type":"code","metadata":{"id":"2_LYZz5nE16C","colab_type":"code","colab":{}},"source":["array_main_cat_property_type = ['Apartment', 'House', 'Condominium', 'Bed & Breakfast', 'Loft', 'Dorm', 'Guesthouse',\n","                               'Chalet', 'Townhouse', 'Hostel', 'Villa']\n","train['Property Type'] = change_cat_to_other(array_main_cat_property_type, train['Property Type'])\n","train['Property Type'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzfLOWsoE16E","colab_type":"text"},"source":["De la misma manera vamos a dejar las categorias de Cancellation Policy en \"strict\", \"flexible\" y \"moderate\", que representan mas del 96% del total y todas las demas las incluiremos en la categoría \"Other\".\n","Usamos nuevamente la funcion change_cat_to_other"]},{"cell_type":"code","metadata":{"id":"WjtFYqe9E16F","colab_type":"code","colab":{}},"source":["array_main_cat_cancellation_policy = ['strict', 'flexible', 'moderate']\n","train['Cancellation Policy'] = change_cat_to_other(array_main_cat_cancellation_policy, train['Cancellation Policy'])\n","train['Cancellation Policy'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9B22siLE16H","colab_type":"text"},"source":["## Codificacion de las variables categoricas (Mean encoding)\n","Una vez reducidas las categorias de las variables categoricas, las convertiremos en numericas mediante el mecanismo de mean\n","encoding.\n","Guardamos las transformacion hechas en Train para reproducirlas en Test con un replace o un map sin volver a aplicar el mean encoding para evitar que los datos de test infulyan en el modelo.\n","Para aplicar le metodo Mean Encoding es conveniente que no haya NaNs en la variable Target (Price), por ello imputaremos esos valores con la media de los precios."]},{"cell_type":"code","metadata":{"id":"qpxF8PHQE16J","colab_type":"code","colab":{}},"source":["y_train_mean = np.mean(train['Price'])\n","train['Price'] = train['Price'].fillna(y_train_mean)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"azFdliYKE16N","colab_type":"code","colab":{}},"source":["#Property Type\n","mean_encode_property_type = train.groupby('Property Type')['Price'].mean()\n","train.loc[:,'Property Type'] = train['Property Type'].map(mean_encode_property_type)\n","\n","#Cancellation Policy \n","mean_encode_cancellation_policy = train.groupby('Cancellation Policy')['Price'].mean()\n","train.loc[:,'Cancellation Policy'] = train['Cancellation Policy'].map(mean_encode_cancellation_policy)\n","\n","#Room Type\n","mean_encode_room_type = train.groupby('Room Type')['Price'].mean()\n","train.loc[:,'Room Type'] = train['Room Type'].map(mean_encode_room_type)\n","\n","#Neighbourhood Cleansed\n","mean_encode_neigh = train.groupby('Neighbourhood Cleansed')['Price'].mean()\n","train.loc[:,'Neighbourhood Cleansed'] = train['Neighbourhood Cleansed'].map(mean_encode_neigh)\n","\n","#Neighbourhood Group Cleansed\n","mean_encode_neigh_group = train.groupby('Neighbourhood Group Cleansed')['Price'].mean()\n","train.loc[:,'Neighbourhood Group Cleansed'] = train['Neighbourhood Group Cleansed'].map(mean_encode_neigh_group)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tmfpP8yJE16V","colab_type":"text"},"source":["## Correlacion de variables\n"]},{"cell_type":"code","metadata":{"id":"Ez67HH3KE16W","colab_type":"code","colab":{}},"source":["import seaborn as sns\n","\n","# Compute the correlation matrix\n","corr = np.abs(train.drop(['Price'], axis=1).corr())\n","\n","# Generate a mask for the upper triangle\n","mask = np.zeros_like(corr, dtype=np.bool)\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Set up the matplotlib figure\n","f, ax = plt.subplots(figsize=(12, 10))\n","\n","# Draw the heatmap with the mask and correct aspect ratio\n","sns.heatmap(corr, mask=mask,vmin = 0.0, vmax=1.0, center=0.5,\n","            linewidths=.1, cmap=\"YlGnBu\", cbar_kws={\"shrink\": .8})\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3S2uZ9GDE16c","colab_type":"text"},"source":["A la vista de la grafica de correlacion y siendo un poco generosos con las variables a eliminar para simplificar el modelo, podemos deducir lo siguiente:\n","1. **Accommodates** tiene una fuerte correlacion con Bedrooms, Beds y moderada con Guests Included\n","2. **Availability 30** tiene fuerte correlacion con Availability 60 Availability 90 y moderada con Availability 365\n","3. **Review Scores Rating** tiene una fuerte correlacion con Review Scores Accuracy, Review Scores Cleanliness, Review Scores Checkin, Review Scores Communication, Review Scores Value y moderada con Review Scores Location.\n","4. **Number of Reviews** tiene fuerte  correlacion con Reviews per Month\n","5. **Neighbourhood Cleansed y Neighbourhood Group Cleansed** muestran una fuerte correlacion, pero de momento dejamos las dos para analizar mediante el filtrado de caracterisdtcas cual de las dos influye mas en la regresion que tenemos que plantear."]},{"cell_type":"code","metadata":{"id":"uBE-xjtdE16d","colab_type":"code","colab":{}},"source":["#Vamos a eliminar todas estas variables del dataset de entrenamiento\n","drop_corr = ['Bedrooms', 'Beds', 'Guests Included', 'Availability 60', 'Availability 90', 'Availability 365',\n","             'Review Scores Accuracy', 'Review Scores Cleanliness', 'Review Scores Checkin', 'Review Scores Communication',\n","             'Review Scores Value', 'Review Scores Location', 'Reviews per Month']\n","train.drop(drop_corr, axis=1, inplace=True)\n","train.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0I3g3WhG44fy","colab_type":"text"},"source":["## Tratamiento de NaNs (Imputacion de nulos)"]},{"cell_type":"markdown","metadata":{"id":"et5VtJh9E16i","colab_type":"text"},"source":["Ningun modelo de Machine o Deep Learning funcionan adecuadamente sin un tratamiento previo de los NaNs de sus registros.\n","\n","Por tanto debemos analizar que variables tienen valores NaN y llevemos a cabo el proceso de imputacion\n","\n","Para hacerlo una opcion seria aplicar los siguientes comandos a todas las variables que quedan:\n","```\n","print(len(train['campo'].unique()))\n","print(train['campo'].isna().sum())\n","print(train['campo'].value_counts())\n","\n","```\n","\n","Afortunadamente contamos con la funion missing_values_table que nos permite visualizar con un unico comando los nulos que aparecen en los distintos campos del dataframe"]},{"cell_type":"code","metadata":{"id":"sj6oK2Kn_zhL","colab_type":"code","colab":{}},"source":["missing_values_table(train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lz_4U9t6Af-Q","colab_type":"text"},"source":["Haciendo esto vemos que los unicos campos que contienen NaNs son:\n","- Bathroom (25) Imputamos la media\n","- Security Deposit (5015). Es mas de la mitad de los regitros por lo que lo eliminamos\n","- Cleaning Fee (3490). Imputamnos un cero. asumismos que quien no tiene ese dato es porque no hay gastos de limpieza.\n","- Review Scores Rating (1768). Imputamos la media. **En su momento consideramos que el no tener este campo relleno no era buena señal por que probamos a imputar el minimo, en lugar de la media, pero el modelo predecia peor, por lo que desestimamos nuestra hipotesis y volvimos a la media**."]},{"cell_type":"code","metadata":{"id":"9hKCP19eE16n","colab_type":"code","colab":{}},"source":["mean_bathroom = np.mean(train['Bathrooms'])\n","mean_review = np.mean(train['Review Scores Rating'])\n","min_review = np.min(train['Review Scores Rating'])\n","train['Bathrooms'] = train['Bathrooms'].fillna(mean_bathroom)\n","train['Cleaning Fee'] = train['Cleaning Fee'].fillna(0)\n","train['Review Scores Rating'] = train['Review Scores Rating'].fillna(mean_review)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQoGmF8mE16j","colab_type":"code","colab":{}},"source":["train.drop('Security Deposit', axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ME1AujcLE16e","colab_type":"text"},"source":["## Separacion y tratamiento de la variable dependiente (variable objetivo o target)\n","Separemos el dataset train entre la varible dependiente (y_train) y el resto de variables independientes (X_train)\n"]},{"cell_type":"code","metadata":{"id":"jkd101-KE16f","colab_type":"code","colab":{}},"source":["y_train = train['Price']\n","X_train= train.drop(['Price'], axis=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2wCnZOJQ-P04","colab_type":"code","colab":{}},"source":["X_train.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yvPWAbcKE16s","colab_type":"code","colab":{}},"source":["y_train.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOjJpjQXE16u","colab_type":"code","colab":{}},"source":["print(len(y_train.unique()))\n","print(y_train.isna().sum())\n","print(y_train.value_counts())\n","print(np.mean(y_train)) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZ3W3as2E164","colab_type":"text"},"source":["## Filtrado para regresion\n","Con las 14 variables que me quedan en X_train aplico los metodos de fitrado **f_regresion** y **mutual_info_regresion**"]},{"cell_type":"code","metadata":{"id":"ylAQnrbBE165","colab_type":"code","colab":{}},"source":["from sklearn.feature_selection import f_regression, mutual_info_regression\n","f_test, _ = f_regression(X_train, y_train)\n","f_test /= np.max(f_test)\n","mi = mutual_info_regression(X_train, y_train)\n","mi /= np.max(mi)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QB9qmYfZE16_","colab_type":"text"},"source":["Mostremos esta informacion en forma de grafica"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"QRMEaVhWE16_","colab_type":"code","colab":{}},"source":["featureNames = list(X_train.columns)\n","\n","plt.figure(figsize=(30, 10))\n","\n","plt.subplot(1,2,1)\n","plt.bar(range(X_train.shape[1]),f_test,  align=\"center\")\n","plt.xticks(range(X_train.shape[1]),featureNames, rotation = 90)\n","plt.xlabel('features')\n","plt.ylabel('Ranking')\n","plt.title('$F-test$ score')\n","\n","\n","plt.subplot(1,2,2)\n","plt.bar(range(X_train.shape[1]),mi,  align=\"center\")\n","plt.xticks(range(X_train.shape[1]),featureNames, rotation = 90)\n","plt.xlabel('features')\n","plt.ylabel('Ranking')\n","plt.title('Mutual information score')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GYGnhyutE17C","colab_type":"text"},"source":["A la vista de las graficas y de los valores de f_test y mi las variables que mas estan impactando en la variable objetivo son:\n","- Neighbourhood Cleansed (quitamos la grupal pues como ya habiamos visto esta fuertemente correlada con esta)\n","- Room Type\n","- Accommodates\n","- Bathrooms\n","- Cleaning Fee\n","- Extra People\n","- Minimum Nigths\n","- Availability 30\n","- cancelation Policy\n","- Review Scores Rating\n","\n","Como era de esperar el barrio, el tipo de habitacion y las personas que pueden ocupar la casa son los parametros que mas afectan al precio de la casa.\n","\n","Por tanto seran estas las variables que usare para testear mis modelos, elimando el resto del dataset de training"]},{"cell_type":"code","metadata":{"id":"-v3ykdhEE17E","colab_type":"code","colab":{}},"source":["drop_filtrado = ['Neighbourhood Group Cleansed', 'Property Type','Maximum Nights', 'Number of Reviews']\n","X_train.drop(drop_filtrado, axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HHZ5j3VDE17H","colab_type":"text"},"source":["Por ultimo cambiare el nombre de la variable \"Neighbourhood Group Cleansed\" por \"Barrio\" para que sea mas manejable"]},{"cell_type":"code","metadata":{"id":"9-6sMfBBE17H","colab_type":"code","colab":{}},"source":["X_train.rename(columns={'Neighbourhood Cleansed':'Barrio'}, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JxHzslWWE17L","colab_type":"code","colab":{}},"source":["X_train.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqY27Pp5E17N","colab_type":"text"},"source":["## Normalizacion\n","Por ultimo llevamos a cabo la normalizacion. Usaremos varias opciones para ver cual es la que mejor se adapta a nuestro modelo:\n","*   StandarEscaler\n","*   MinMax\n","\n","\n","Recordemos que para test deberemos usar el scaler obtenido en train\n"]},{"cell_type":"code","metadata":{"id":"V8yaa2QQE17O","colab_type":"code","colab":{}},"source":["from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","\n","#StandardScaler\n","standardScalerX = preprocessing.StandardScaler().fit(X_train)\n","XtrainStdScaled = standardScalerX.transform(X_train)\n","\n","standardScalerY = preprocessing.StandardScaler().fit(np.array(y_train).reshape(-1, 1))\n","YtrainStdScaled = standardScalerY.transform(np.array(y_train).reshape(-1, 1))\n","\n","#MinMax\n","cs = MinMaxScaler()\n","XtrainMmScaled = cs.fit_transform(X_train)\n","\n","maxPrice = train[\"Price\"].max()\n","YtrainMmScaled = np.array(y_train).reshape(-1, 1)/maxPrice\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eOhimm80E17R","colab_type":"text"},"source":["**Y hasta aqui el analisis exploratorio y la limpieza del dataset, empezamos con las pruebas de los modelos**"]},{"cell_type":"markdown","metadata":{"id":"UA6RtC0NE17T","colab_type":"text"},"source":["## Preparacion datos Test"]},{"cell_type":"markdown","metadata":{"id":"chcP9vzCE17U","colab_type":"text"},"source":["Antes de iniciar la evaluacion de modelos, vamos a dejar preparado el **dataset de Test** con las mismas transformaciones que hemos llevado a cabo sobre el de Train.\n","Abajo indicamos el listado de dichas transformaciones para no olvidarnos de ninguna:\n","- Lo primero es el dropeo de las variables que no van a participar en el modelo (correlacion y filtrado): drop_corr y drop_filtrado.\n","- Drop de \"Security Deposit\"\n","- Tratamiento de las variables categoricas que influyen en el modelo (filtrado de categorias y Encoder): mean_encode_room_type, mean_encode_neigh y mean_encode_cancellation_policy\n","- Imputacion  de NaNs: Price, Bathroom y Cleaning Fee\n"]},{"cell_type":"markdown","metadata":{"id":"r7oepJFlE17V","colab_type":"text"},"source":["### Analizamos y limpiamos el dataset de test"]},{"cell_type":"code","metadata":{"id":"0WUsZWuaE17W","colab_type":"code","colab":{}},"source":["test = pd.read_csv('/content/gdrive/My Drive/Public/Practica_Deep_Learning_DA/data/test_clean.csv',sep=';', decimal='.')\n","test.shape\n","#test.info()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrwCwT8xE17Y","colab_type":"code","colab":{}},"source":["missing_values_table(test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"al7kOroPE17b","colab_type":"code","colab":{}},"source":["#Dropeamos\n","test.drop(drop_corr, axis=1, inplace=True)\n","test.drop(drop_filtrado, axis=1, inplace=True)\n","\n","#Eliminamos tambien la variable Security Deposit \n","test.drop('Security Deposit', axis=1, inplace=True)\n","\n","#imputamos los valores NaNs del target con la media del train (y_train_mean)\n","media_target_test = np.mean(test['Price'])\n","test['Price'] = test['Price'].fillna(y_train_mean)\n","\n","#Imputamos los NaNs de la misma manera que se hace en Train\n","test['Bathrooms'] = test['Bathrooms'].fillna(mean_bathroom)\n","test['Cleaning Fee'] = test['Cleaning Fee'].fillna(0)\n","test['Review Scores Rating'] = test['Review Scores Rating'].fillna(mean_review)\n","\n","\n","#Aplicamos el Mean Encoder a \"Room Type\", \"Neighbourhood Cleansed\" y \"Cancellation Policy\"  el obtenido en train, si aparece alguna categoria nueva en test\n","#lo trataremos a posteriori\n","#mean_encode_room_type_test = test.groupby('Room Type')['Price'].mean()\n","#mean_encode_neigh_test = test.groupby('Neighbourhood Cleansed')['Price'].mean()\n","#mean_encode_cancellation_policy_test = test.groupby('Cancellation Policy')['Price'].mean()\n","test.loc[:,'Room Type'] = test['Room Type'].map(mean_encode_room_type)\n","test.loc[:,'Neighbourhood Cleansed'] = test['Neighbourhood Cleansed'].map(mean_encode_neigh)\n","test.loc[:,'Cancellation Policy'] = test['Cancellation Policy'].map(mean_encode_cancellation_policy)\n","\n","\n","#Por ultimo cambiamos el nombre de la variable a Barrio\n","test.rename(columns={'Neighbourhood Cleansed':'Barrio'}, inplace=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"CYOXXc4JE17d","colab_type":"code","colab":{}},"source":["missing_values_table(test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wl8qUDYDE17e","colab_type":"text"},"source":["Como quedan valores nulos en Barrio y Cancellation Policy debido a aplicar sobre ellas el Mean Encoding de train, lo rellenaremos con la media de las medias obtenidas para esas categoria"]},{"cell_type":"code","metadata":{"id":"eC363dAUE17f","colab_type":"code","colab":{}},"source":["mean_mean_encode_neigh = np.mean(mean_encode_neigh)\n","mean_mean_encode_cancellation_policy = np.mean(mean_encode_cancellation_policy)\n","test['Barrio'].fillna(mean_mean_encode_neigh, inplace=True)\n","test['Cancellation Policy'].fillna(mean_mean_encode_cancellation_policy, inplace=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTbFfOniE17j","colab_type":"text"},"source":["### Separamos la variable objetivo del tets"]},{"cell_type":"code","metadata":{"id":"StKv43BHE17k","colab_type":"code","colab":{}},"source":["y_test = test['Price']\n","X_test= test.drop(['Price'], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8q9UMlcLE17r","colab_type":"text"},"source":["Normalizamos los datos de Test usando las normalizaciones de Train"]},{"cell_type":"code","metadata":{"id":"HcZHBIPxE17r","colab_type":"code","colab":{}},"source":["#StandardScaler\n","XtestStdScaled = standardScalerX.transform(X_test)\n","YtestStdScaled = standardScalerY.transform(np.array(y_test).reshape(-1, 1))\n","\n","#MinMax\n","cs = MinMaxScaler()\n","XtestMmScaled = cs.fit_transform(X_test)\n","\n","YtestMmScaled = np.array(y_test).reshape(-1, 1)/maxPrice\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oInRILSNVGa","colab_type":"code","colab":{}},"source":["# Normalizamos los precios de las casas a [0,1] usando el maximo de los precios del conjunto de train\n","maxPrice = X_train[\"Price\"].max()\n","trainY = X_train[\"Price\"] / maxPrice\n","testY = X_test[\"Price\"] / maxPrice"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3YZgusQQosU","colab_type":"code","colab":{}},"source":["X_test.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pilsh14y0ydu","colab_type":"text"},"source":["# Mixed Input Net\n"]},{"cell_type":"code","metadata":{"id":"aITzRjk702sv","colab_type":"code","colab":{}},"source":["# Llevamos a cabo los import necesarios\n","from keras.layers.core import Dense\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.layers import concatenate\n","\n","# Creamos las redes nn y cnn\n","nn = create_nn(X_train.shape[1], regress=False)\n","cnn = create_cnn(216, 144, 3, regress=False)\n","\n","# Concatenamos la salida de cnn y nn que se convertira en la entrada de nuestro conjunto final de capas de prediccion\n","combinedInput = concatenate([nn.output, cnn.output])\n","\n","# La capa final \"Fully Connected\" estara formada por dos capas densas, la ultima sera la que llevara a cabo la prediccion.\n","x = Dense(4, activation=\"relu\")(combinedInput)\n","x = Dense(1, activation=\"linear\")(x)\n","\n","# Nuestro modelo final aceptara datos categoricos/numericos en la red tradicional (nn)\n","# e imagenes en la red convolucional (cnn), generando un valor escalar que \n","#representara la prediccion del valor de la casa \n","model = Model(inputs=[nn.input, cnn.input], outputs=x)\n","\n","# Compilamos el modelo usando \"mean_absolute_percentage_error\" como funcion de perdida\n","opt = Adam(lr=0.001, decay=1e-3 / 200)\n","model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n","\n","# Entrenamos el modelo\n","#Usamos los datos categoricos/numericos sin normalizar porque como hemos podido comprobar en nn_regresion_02 dan mejor resultado que los normalizados\n","n_epochs = 100\n","print(\"[INFO] training model...\")\n","history_callback = model.fit(\n","\t[XtrainStdScaled, trainImagesX], YtrainStdScaled,\n","\tvalidation_data=([XtestStdScaled, testImagesX], YtestStdScaled),\n","\tepochs=n_epochs, batch_size=64)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G1IhA87J7ohp","colab_type":"text"},"source":["### Curva de perdidas"]},{"cell_type":"code","metadata":{"id":"Fsj5lZ_wYlNR","colab_type":"code","colab":{}},"source":["# veamos nuestra curva de pérdidas\n","plt.plot(np.arange(0, n_epochs), history_callback.history[\"loss\"])\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8K5ab1HUlmYu","colab_type":"text"},"source":["A la vista de estos datos observamos una curva con muchas mas irregularidades y que tarda mas en converger.\n"]},{"cell_type":"markdown","metadata":{"id":"PPA_TyTyxn9I","colab_type":"text"},"source":["### Bondad del modelo"]},{"cell_type":"code","metadata":{"id":"70KpaEUKO-dM","colab_type":"code","colab":{}},"source":["# Obtenemos la prediccion de nuestro modelo para los datos de Test\n","preds = model.predict([X_test, testImagesX])\n"," \n","\n","# Calulamos la diferencia entre los precios predichos y los reales\n","# A continuacion calcularemos la diferencia porcentual y la diferecia absouta en porcentaje\n","diff = preds.flatten() - testY\n","percentDiff = (diff / testY) * 100\n","absPercentDiff = np.abs(percentDiff)\n"," \n","\n","#Calculamos la media y la desviacion estandar\n","mean = np.mean(absPercentDiff)\n","std = np.std(absPercentDiff)\n"," \n","# Mostramos las estadisticas de nuestro modelo.\n","print(\"Precio medio de las pisos: {:.2f}€, Desviacion standar: {:.2f}€\".format(\n","    full_airbnb_images[\"Price\"].mean(), full_airbnb_images[\"Price\"].std()))\n","print(\"Modelo --> mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3HWB3tcny1Y2","colab":{}},"source":["history_callback.history[\"loss\"]"],"execution_count":0,"outputs":[]}]}